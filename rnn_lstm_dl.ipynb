{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgFjeaWVphfM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data, california.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert arrays to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(RNNRegressor, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x[:, -1, :])  # Using last sequence output\n",
        "        return x\n",
        "\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(LSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])  # Using last sequence output\n",
        "        return x"
      ],
      "metadata": {
        "id": "wR6lddDCzV9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for features, targets in train_loader:\n",
        "            features = features.unsqueeze(1)  # Add sequence dimension\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in test_loader:\n",
        "            features = features.unsqueeze(1)  # Add sequence dimension\n",
        "            outputs = model(features).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(test_loader)"
      ],
      "metadata": {
        "id": "pC3Higxf0MoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 64\n",
        "\n",
        "# Define the models, optimizers, and loss function\n",
        "rnn_model = RNNRegressor(input_dim, hidden_dim)\n",
        "rnn_optimizer = torch.optim.Adam(rnn_model.parameters())\n",
        "rnn_criterion = nn.MSELoss()\n",
        "\n",
        "lstm_model = LSTMRegressor(input_dim, hidden_dim)\n",
        "lstm_optimizer = torch.optim.Adam(lstm_model.parameters())\n",
        "lstm_criterion = nn.MSELoss()\n",
        "\n",
        "# Train and evaluate RNN\n",
        "train_model(rnn_model, train_loader, rnn_criterion, rnn_optimizer, num_epochs=50)\n",
        "rnn_mse = evaluate_model(rnn_model, test_loader, rnn_criterion)  # Pass criterion here\n",
        "print(f\"RNN Test MSE: {rnn_mse}\")\n",
        "\n",
        "# Train and evaluate LSTM\n",
        "train_model(lstm_model, train_loader, lstm_criterion, lstm_optimizer, num_epochs=50)\n",
        "lstm_mse = evaluate_model(lstm_model, test_loader, lstm_criterion)  # Pass criterion here\n",
        "print(f\"LSTM Test MSE: {lstm_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiCz5d970PPF",
        "outputId": "7b12ac4c-a3b5-4eaa-98dc-6f635eae457b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7848940644153329\n",
            "Epoch 2, Loss: 0.4881038557766944\n",
            "Epoch 3, Loss: 0.47521566996682985\n",
            "Epoch 4, Loss: 0.46073768360814615\n",
            "Epoch 5, Loss: 0.440802519808906\n",
            "Epoch 6, Loss: 0.42219798305873263\n",
            "Epoch 7, Loss: 0.4081151943235088\n",
            "Epoch 8, Loss: 0.3968458482237576\n",
            "Epoch 9, Loss: 0.3880287832869289\n",
            "Epoch 10, Loss: 0.3801048655503307\n",
            "Epoch 11, Loss: 0.3732105886722489\n",
            "Epoch 12, Loss: 0.367247852785585\n",
            "Epoch 13, Loss: 0.3636733550802162\n",
            "Epoch 14, Loss: 0.35805700771337334\n",
            "Epoch 15, Loss: 0.3530645897258218\n",
            "Epoch 16, Loss: 0.3492753896898366\n",
            "Epoch 17, Loss: 0.34458413422324286\n",
            "Epoch 18, Loss: 0.3408816324622739\n",
            "Epoch 19, Loss: 0.33687162207973786\n",
            "Epoch 20, Loss: 0.3361155196677807\n",
            "Epoch 21, Loss: 0.3315263386570321\n",
            "Epoch 22, Loss: 0.330221077492244\n",
            "Epoch 23, Loss: 0.32795047430437085\n",
            "Epoch 24, Loss: 0.32529158066857117\n",
            "Epoch 25, Loss: 0.3251936472627486\n",
            "Epoch 26, Loss: 0.32352246843321725\n",
            "Epoch 27, Loss: 0.32232518376811414\n",
            "Epoch 28, Loss: 0.32100682423356197\n",
            "Epoch 29, Loss: 0.3194650378973447\n",
            "Epoch 30, Loss: 0.31901981967118703\n",
            "Epoch 31, Loss: 0.31852720128444506\n",
            "Epoch 32, Loss: 0.31735881691610857\n",
            "Epoch 33, Loss: 0.31638193235898665\n",
            "Epoch 34, Loss: 0.3154622650589765\n",
            "Epoch 35, Loss: 0.31535612019284287\n",
            "Epoch 36, Loss: 0.313964342063134\n",
            "Epoch 37, Loss: 0.31312980419458\n",
            "Epoch 38, Loss: 0.31285637812039185\n",
            "Epoch 39, Loss: 0.311651871788727\n",
            "Epoch 40, Loss: 0.3119564595949916\n",
            "Epoch 41, Loss: 0.31219142468920513\n",
            "Epoch 42, Loss: 0.3105081458424413\n",
            "Epoch 43, Loss: 0.31107930587099275\n",
            "Epoch 44, Loss: 0.31012315766582654\n",
            "Epoch 45, Loss: 0.310150775773263\n",
            "Epoch 46, Loss: 0.3089333073228829\n",
            "Epoch 47, Loss: 0.30846856375289866\n",
            "Epoch 48, Loss: 0.3073245867213249\n",
            "Epoch 49, Loss: 0.308406422914605\n",
            "Epoch 50, Loss: 0.3067810459806071\n",
            "RNN Test MSE: 0.32742237400754476\n",
            "Epoch 1, Loss: 0.9188263036541698\n",
            "Epoch 2, Loss: 0.41429647194337127\n",
            "Epoch 3, Loss: 0.3911437508036462\n",
            "Epoch 4, Loss: 0.3789735148668058\n",
            "Epoch 5, Loss: 0.37004945251744154\n",
            "Epoch 6, Loss: 0.36425518131518897\n",
            "Epoch 7, Loss: 0.35932830271361643\n",
            "Epoch 8, Loss: 0.3534677905362648\n",
            "Epoch 9, Loss: 0.3493612940286019\n",
            "Epoch 10, Loss: 0.34658658912029955\n",
            "Epoch 11, Loss: 0.3409392042482604\n",
            "Epoch 12, Loss: 0.33430304443247094\n",
            "Epoch 13, Loss: 0.32806402339073\n",
            "Epoch 14, Loss: 0.3223291318610946\n",
            "Epoch 15, Loss: 0.31683296005263234\n",
            "Epoch 16, Loss: 0.31294082304419474\n",
            "Epoch 17, Loss: 0.309447766888107\n",
            "Epoch 18, Loss: 0.3076570145316537\n",
            "Epoch 19, Loss: 0.3058745105274368\n",
            "Epoch 20, Loss: 0.3045224569532154\n",
            "Epoch 21, Loss: 0.3024944151686721\n",
            "Epoch 22, Loss: 0.30070142747199\n",
            "Epoch 23, Loss: 0.29967164525098\n",
            "Epoch 24, Loss: 0.29868961930231647\n",
            "Epoch 25, Loss: 0.29723982341125493\n",
            "Epoch 26, Loss: 0.29546160146940587\n",
            "Epoch 27, Loss: 0.2950846264463698\n",
            "Epoch 28, Loss: 0.2942993714647411\n",
            "Epoch 29, Loss: 0.291961735507921\n",
            "Epoch 30, Loss: 0.29169303197197094\n",
            "Epoch 31, Loss: 0.2905877906533117\n",
            "Epoch 32, Loss: 0.2891440084186029\n",
            "Epoch 33, Loss: 0.28825561809461825\n",
            "Epoch 34, Loss: 0.28758372968900225\n",
            "Epoch 35, Loss: 0.28689977056310734\n",
            "Epoch 36, Loss: 0.2845845686004663\n",
            "Epoch 37, Loss: 0.2846592917800123\n",
            "Epoch 38, Loss: 0.2836352318413411\n",
            "Epoch 39, Loss: 0.2833606390491879\n",
            "Epoch 40, Loss: 0.28267196763682273\n",
            "Epoch 41, Loss: 0.28064609429972287\n",
            "Epoch 42, Loss: 0.28133302650508263\n",
            "Epoch 43, Loss: 0.2794474537426591\n",
            "Epoch 44, Loss: 0.27832261800888725\n",
            "Epoch 45, Loss: 0.2781932638640436\n",
            "Epoch 46, Loss: 0.2777383958689811\n",
            "Epoch 47, Loss: 0.2768217047871372\n",
            "Epoch 48, Loss: 0.2758257720536415\n",
            "Epoch 49, Loss: 0.27453788177492894\n",
            "Epoch 50, Loss: 0.27450412331156204\n",
            "LSTM Test MSE: 0.2942033680228069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCxiGVyz0RWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}